{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594dfa49-7953-4e42-9607-639ea7f91f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 07:31:51.656601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-23 07:31:52.464027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-10-23 07:31:53.364668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.393766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.394073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.396073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.396301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.396505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.911674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.911956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.912172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-23 07:31:53.912351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14118 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:81:00.0, compute capability: 8.9\n",
      "2023-10-23 07:31:54.534769: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2023-10-23 07:31:54.565122: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    }
   ],
   "source": [
    "from dichasus_cf0x import training_set, test_set, spec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e2035b-c9ac-417a-b8fd-538a7679b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csi_time_domain(csi, pos, time):\n",
    "    csi = tf.signal.fftshift(tf.signal.ifft(tf.signal.fftshift(csi, axes = -1)), axes = -1)\n",
    "    return csi, pos, time\n",
    "\n",
    "def cut_out_taps(tap_start, tap_stop):\n",
    "    def cut_out_taps_func(csi, pos, time):\n",
    "        return csi[:,:,:,tap_start:tap_stop], pos, time\n",
    "\n",
    "    return cut_out_taps_func\n",
    "\n",
    "\n",
    "training_set = training_set.map(csi_time_domain, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "training_set = training_set.map(cut_out_taps(507, 520), num_parallel_calls = tf.data.AUTOTUNE)\n",
    "\n",
    "test_set = test_set.map(csi_time_domain, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "test_set = test_set.map(cut_out_taps(507, 520), num_parallel_calls = tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c40d1-bd56-4147-8cbf-e7ad3f1c93b0",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd050d8-1d7c-479d-91e5-95a9086c6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dissimilarity_matrix_geodesic_meters = np.load(\"results/dissimilarity_matrix_geodesic_meters.npy\")\n",
    "classical_estimated_positions = np.load(\"results/estimated_positions.npy\")\n",
    "estimated_aoas = np.load(\"results/estimated_aoas.npy\")\n",
    "estimated_toas = np.load(\"results/estimated_toas.npy\")\n",
    "delayspreads = np.load(\"results/delayspreads.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7669356-8ebc-484b-b4f7-cc0e4a649693",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_positions = []\n",
    "csi_time_domain = []\n",
    "\n",
    "for csi, pos, time in training_set.prefetch(tf.data.AUTOTUNE).batch(1000):\n",
    "    csi_time_domain.append(csi.numpy())\n",
    "    groundtruth_positions.append(pos.numpy())\n",
    "\n",
    "csi_time_domain = np.concatenate(csi_time_domain)\n",
    "groundtruth_positions = np.concatenate(groundtruth_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4105bbc2-ba40-49ef-ad6c-a9a0fc9d0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that z-coordinate of TX is constant and known\n",
    "HEIGHT = np.mean(groundtruth_positions[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a342b9fe-99fc-4b92-b612-afc059238150",
   "metadata": {},
   "source": [
    "# Manifold Learning: Finding the Low-Dimensional Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89ffa60-6d50-4058-83d7-7565f39bb39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineeringLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(FeatureEngineeringLayer, self).__init__(dtype = tf.complex64)\n",
    "\n",
    "    def call(self, csi):\n",
    "        # Compute sample autocorrelations for any combination of two antennas in the whole system\n",
    "        # for the same datapoint and time tap.\n",
    "        # csi has shape (batchsize, array, antenna row, antenna column, tap)\n",
    "        sample_autocorrelations = tf.einsum(\"darmt,dbsnt->dtabrmsn\", csi, tf.math.conj(csi))\n",
    "        return tf.stack([tf.math.real(sample_autocorrelations), tf.math.imag(sample_autocorrelations)], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990a0fce-6456-4a62-81fb-323505709e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 07:32:13.315692: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1763496036 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "csi_time_domain_tensor = tf.constant(csi_time_domain)\n",
    "dissimilarity_matrix_geodesic_tensor = tf.constant(dissimilarity_matrix_geodesic_meters, dtype = tf.float32)\n",
    "estimated_aoas_tensor = tf.constant(estimated_aoas, dtype = tf.float32)\n",
    "estimated_toas_tensor = tf.constant(estimated_toas, dtype = tf.float32)\n",
    "delayspreads_tensor = tf.constant(delayspreads, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbaec5c3-8aa7-4dfa-a682-e3327dc8d386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 07:32:14.497382: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1763496036 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "datapoint_count = tf.shape(csi_time_domain_tensor)[0].numpy()\n",
    "\n",
    "random_integer_pairs_dataset = tf.data.Dataset.zip(tf.data.Dataset.random(), tf.data.Dataset.random())\n",
    "\n",
    "@tf.function\n",
    "def fill_pairs(randA, randB):\n",
    "    indexA = randA % datapoint_count\n",
    "    indexB = randB % datapoint_count\n",
    "    input = (csi_time_domain_tensor[indexA], csi_time_domain_tensor[indexB])\n",
    "    labels = tf.concat([[dissimilarity_matrix_geodesic_tensor[indexA, indexB]], estimated_aoas_tensor[indexA], estimated_aoas_tensor[indexB], estimated_toas_tensor[indexA], estimated_toas_tensor[indexB], delayspreads_tensor[indexA], delayspreads_tensor[indexB]], 0)\n",
    "    return input, labels\n",
    "\n",
    "random_pair_dataset = random_integer_pairs_dataset.map(fill_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7dcb4c-ac3a-4128-8432-3cd6664beeba",
   "metadata": {},
   "source": [
    "### The Forward Charting Function: Defining the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4373f46-80c9-471b-b674-1a0d17cdef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_count = np.shape(csi_time_domain)[1]\n",
    "rows_per_array_count = np.shape(csi_time_domain)[2]\n",
    "antennas_per_row_count = np.shape(csi_time_domain)[3]\n",
    "tap_count = np.shape(csi_time_domain)[4]\n",
    "\n",
    "cc_embmodel_input = tf.keras.Input(shape = (array_count, rows_per_array_count, antennas_per_row_count, tap_count), name=\"input\", dtype = tf.complex64)\n",
    "cc_embmodel_output = FeatureEngineeringLayer()(cc_embmodel_input)\n",
    "cc_embmodel_output = tf.keras.layers.Flatten()(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.Dense(1024, activation = \"relu\")(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.Dense(512, activation = \"relu\")(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.Dense(256, activation = \"relu\")(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.Dense(128, activation = \"relu\")(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.Dense(64, activation = \"relu\")(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.BatchNormalization()(cc_embmodel_output)\n",
    "cc_embmodel_output = tf.keras.layers.Dense(2, activation = \"linear\")(cc_embmodel_output)\n",
    "\n",
    "cc_embmodel = tf.keras.Model(inputs = cc_embmodel_input, outputs = cc_embmodel_output, name = \"ForwardChartingFunction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712df8f-46e5-4290-a4de-0ed97e23cb3f",
   "metadata": {},
   "source": [
    "# Loss function including AoA / ToA Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa681648-ce49-4eb0-8880-a963db272bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEED_OF_LIGHT = 299792458\n",
    "BANDWIDTH = 50e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b11f5e-c1d7-4a92-9dbf-44581396ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are some empirically determined heuristics, may be further tweaked\n",
    "def get_aoa_vonmises_kappas(delayspreads):\n",
    "    return 0.0000003 / (delayspreads + 0.025e-7)\n",
    "\n",
    "def get_toa_variances(delayspreads):\n",
    "    return 1 + (delayspreads * 1e7) ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd238ff-5e7e-4544-8342-a6fe82fd0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_positions = np.zeros((len(spec[\"antennas\"]), 3))\n",
    "array_normalvectors = np.zeros((len(spec[\"antennas\"]), 3))\n",
    "\n",
    "for antidx, antenna in enumerate(spec[\"antennas\"]):\n",
    "    array_positions[antidx] = np.asarray(antenna[\"location\"])\n",
    "    array_normalvectors[antidx] = np.asarray(antenna[\"direction\"])\n",
    "\n",
    "array_positions_tensor = tf.constant(array_positions, dtype = tf.float32)\n",
    "array_normalvectors_tensor = tf.constant(array_normalvectors, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f34abc-3872-4acf-9fb3-c9c30e789cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical AoA/TDoA-based likelihood function\n",
    "def classical_likelihood_func_vectorized(pos, aoas, toas, rms_delay_spreads):\n",
    "    pos_with_height = tf.concat([pos, HEIGHT * tf.ones(tf.shape(pos)[0])[:, tf.newaxis]], 1)\n",
    "\n",
    "    # \"relative\" has shape (number of positions, number of arrays, 3 spatial dimensions)\n",
    "    relative_pos = pos_with_height[:,tf.newaxis,:] - array_positions_tensor\n",
    "\n",
    "    # Compute ideal 2D AoAs at given positions, shape: (number of positions, number of arrays)\n",
    "    ideal_aoas = tf.math.atan2(-relative_pos[:, :, 1], -relative_pos[:, :, 0]) - tf.math.atan2(-array_normalvectors_tensor[:, 1], -array_normalvectors_tensor[:, 0])\n",
    "\n",
    "    # Compute ideal TDoAs at given positions, shape: (number of positions, number of arrays)\n",
    "    ideal_toas = tf.sqrt(tf.math.reduce_sum(tf.square(relative_pos), axis = -1)) / SPEED_OF_LIGHT * BANDWIDTH\n",
    "\n",
    "    # Compute AoA likelihoods based on von Mises distribution\n",
    "    kappa = get_aoa_vonmises_kappas(rms_delay_spreads)\n",
    "    aoa_likelihoods = tf.exp(kappa * tf.cos(ideal_aoas - aoas)) / (2 * np.pi * tf.math.bessel_i0(kappa))\n",
    "\n",
    "    # Compute TDoA likelihoods based on Gaussian distribution. tdoa_difference has shape (number of positions, number of array-pairs)\n",
    "    arraysA, arraysB = np.triu_indices(len(spec[\"antennas\"]), k = 1)\n",
    "    estimated_tdoas = tf.gather(toas, arraysB, axis = 1) - tf.gather(toas, arraysA, axis = 1)\n",
    "    ideal_tdoas = tf.gather(ideal_toas, arraysB, axis = 1) - tf.gather(ideal_toas, arraysA, axis = 1)\n",
    "    tdoa_difference = ideal_tdoas - estimated_tdoas\n",
    "    variances = get_toa_variances(tf.math.maximum(tf.gather(rms_delay_spreads, arraysA, axis = 1), tf.gather(rms_delay_spreads, arraysB, axis = 1)))\n",
    "    toa_likelihoods = tf.multiply(tf.divide(1, tf.sqrt(2 * np.pi * variances)), tf.exp(-0.5 * tf.divide(tf.square(tdoa_difference), variances)))\n",
    "\n",
    "    return tf.multiply(tf.math.reduce_prod(aoa_likelihoods, axis = -1), tf.math.reduce_prod(toa_likelihoods, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e3eef9-cba9-4aab-a037-cc1deabf5ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_siamese_classical_loss(classical_weight):\n",
    "    def combined_siamese_classical_loss(y_true, y_pred):\n",
    "        dissimilarities = y_true[:,0]\n",
    "    \n",
    "        pos_A, pos_B = (y_pred[:,:2], y_pred[:,2:])\n",
    "        distances_pred = tf.math.reduce_euclidean_norm(pos_A - pos_B, axis = 1)\n",
    "        siamese_loss = tf.reduce_mean(tf.square(distances_pred - dissimilarities))\n",
    "    \n",
    "        aoa_A = y_true[:,1:5]\n",
    "        aoa_B = y_true[:,5:9]\n",
    "        toa_A = y_true[:,9:13]\n",
    "        toa_B = y_true[:,13:17]\n",
    "        ds_A = y_true[:,17:21]\n",
    "        ds_B = y_true[:,21:25]\n",
    "        classical_loss = -tf.reduce_sum(classical_likelihood_func_vectorized(pos_A, aoa_A, toa_A, ds_A) + classical_likelihood_func_vectorized(pos_B, aoa_B, toa_B, ds_B))\n",
    "        \n",
    "        return classical_weight * classical_loss + (1 - classical_weight) * siamese_loss\n",
    "\n",
    "    return combined_siamese_classical_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb754631-ddb8-43d3-9a91-e3ca94f997d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = tf.keras.layers.Input(shape = (array_count, rows_per_array_count, antennas_per_row_count, tap_count,), dtype = tf.complex64)\n",
    "input_B = tf.keras.layers.Input(shape = (array_count, rows_per_array_count, antennas_per_row_count, tap_count,), dtype = tf.complex64)\n",
    "\n",
    "embedding_A = cc_embmodel(input_A)\n",
    "embedding_B = cc_embmodel(input_B)\n",
    "\n",
    "output = tf.keras.layers.concatenate([embedding_A, embedding_B], axis=1)\n",
    "model = tf.keras.models.Model([input_A, input_B], output, name = \"SiameseNeuralNetwork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495c438-37a7-4295-85a1-461874324199",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094391bd-094e-4c2d-bdee-b917a6a36994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Session  1 \n",
      "Batch Size:  400 \n",
      "Learning rate:  0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 07:32:15.508992: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1763496036 exceeds 10% of free system memory.\n",
      "2023-10-23 07:32:20.544080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-10-23 07:32:20.570487: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55dac9319640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-23 07:32:20.570538: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4080, Compute Capability 8.9\n",
      "2023-10-23 07:32:20.579838: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-23 07:32:20.744398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-10-23 07:32:20.860858: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 13s 13ms/step - loss: -0.1442\n",
      "657/657 [==============================] - 1s 2ms/step\n",
      "Mean Absolute Error (MAE): 0.5370m\n",
      "\n",
      "Training Session  2 \n",
      "Batch Size:  800 \n",
      "Learning rate:  0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 07:32:31.241365: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1763496036 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6s 15ms/step - loss: -0.2627\n",
      "657/657 [==============================] - 1s 2ms/step\n",
      "Mean Absolute Error (MAE): 0.5520m\n",
      "\n",
      "Training Session  3 \n",
      "Batch Size:  1200 \n",
      "Learning rate:  0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 07:32:38.933613: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1763496036 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 5s 21ms/step - loss: -0.4316\n",
      "657/657 [==============================] - 1s 2ms/step\n",
      "Mean Absolute Error (MAE): 0.4355m\n",
      "\n",
      "Training Session  4 \n",
      "Batch Size:  2000 \n",
      "Learning rate:  0.001\n",
      "100/100 [==============================] - 5s 36ms/step - loss: -0.7527\n",
      "657/657 [==============================] - 1s 1ms/step\n",
      "Mean Absolute Error (MAE): 0.4367m\n",
      "\n",
      "Training Session  5 \n",
      "Batch Size:  3000 \n",
      "Learning rate:  0.0005\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Feel free to tweak these training hyperparameters - a good choice of these paremters is important for performance!\n",
    "samples_per_session = 200000\n",
    "learning_rates = [5e-3, 2e-3, 1e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
    "batch_sizes = [400, 800, 1200, 2000, 3000, 3000, 4000, 5000]\n",
    "classical_weights = [0.98, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85]\n",
    "\n",
    "\n",
    "for session, learning_rate, batch_size, classical_weight in zip(range(len(learning_rates)), learning_rates, batch_sizes, classical_weights):\n",
    "    print(\"\\nTraining Session \", session + 1, \"\\nBatch Size: \", batch_size, \"\\nLearning rate: \", learning_rate)\n",
    "\n",
    "    # Fit model\n",
    "    model.compile(loss = get_combined_siamese_classical_loss(classical_weight), optimizer = optimizer)\n",
    "    optimizer.learning_rate.assign(learning_rate)\n",
    "    steps_per_epoch = int(samples_per_session / batch_size)\n",
    "    model.fit(random_pair_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE), steps_per_epoch = samples_per_session / batch_size)\n",
    "\n",
    "    # Quick evaluation\n",
    "    channel_chart_positions = cc_embmodel.predict(csi_time_domain_tensor)\n",
    "    errorvectors = groundtruth_positions[:,:2] - channel_chart_positions\n",
    "    print(f\"Mean Absolute Error (MAE): {np.mean(np.sqrt(errorvectors[:,0]**2 + errorvectors[:,1]**2)):.4f}m\")\n",
    "\n",
    "    #plot_colorized(channel_chart_positions, groundtruth_positions, title = \"Siamese-Based Channel Chart - Training Set\")\n",
    "    #plot_quiver(channel_chart_positions, groundtruth_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d255c8-b76a-4615-9336-b6a77362a107",
   "metadata": {},
   "source": [
    "# Optional: Evaluation, Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd70bb-bead-4154-9451-78fe8db6c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53099af1-e0ff-4920-89f7-4e5606c753b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colorized(positions, reference_positions, title = None, show = True, alpha = 1.0):\n",
    "    # Generate RGB colors for datapoints\n",
    "    center_point = np.zeros(2, dtype = np.float32)\n",
    "    center_point[0] = 0.5 * (np.min(reference_positions[:, 0], axis = 0) + np.max(reference_positions[:, 0], axis = 0))\n",
    "    center_point[1] = 0.5 * (np.min(reference_positions[:, 1], axis = 0) + np.max(reference_positions[:, 1], axis = 0))\n",
    "    NormalizeData = lambda in_data : (in_data - np.min(in_data)) / (np.max(in_data) - np.min(in_data))\n",
    "    rgb_values = np.zeros((reference_positions.shape[0], 3))\n",
    "    rgb_values[:, 0] = 1 - 0.9 * NormalizeData(reference_positions[:, 0])\n",
    "    rgb_values[:, 1] = 0.8 * NormalizeData(np.square(np.linalg.norm(reference_positions[:,:2] - center_point, axis=1)))\n",
    "    rgb_values[:, 2] = 0.9 * NormalizeData(reference_positions[:, 1])\n",
    "\n",
    "    # Plot datapoints\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=16)\n",
    "    plt.scatter(positions[:, 0], positions[:, 1], c = rgb_values, alpha = alpha, s = 10, linewidths = 0)\n",
    "    plt.xlabel(\"x coordinate\")\n",
    "    plt.ylabel(\"y coordinate\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_quiver(positions, reference_positions, title = None, nth_errorvector = 15):\n",
    "    errorvectors = reference_positions[:,:2] - positions\n",
    "    errors = np.sqrt(errorvectors[:,0]**2 + errorvectors[:,1]**2)\n",
    "    mae = np.mean(errors)\n",
    "    \n",
    "    fulltitle = f\"Error Vectors, MAE = {mae:.4f}m\"\n",
    "    if title is not None:\n",
    "        fulltitle = title + \" - \" + fulltitle\n",
    "    plot_colorized(positions, reference_positions, title = fulltitle, show = False, alpha = 0.3)\n",
    "    plt.quiver(positions[::nth_errorvector, 0], positions[::nth_errorvector, 1], errorvectors[::nth_errorvector, 0], errorvectors[::nth_errorvector, 1], color = \"black\", angles = \"xy\", scale_units = \"xy\", scale = 1)\n",
    "    plt.show()\n",
    "\n",
    "def plot_cdf(positions, reference_positions, title = None):\n",
    "    errorvectors = reference_positions[:,:2] - positions\n",
    "    errors = np.sqrt(errorvectors[:,0]**2 + errorvectors[:,1]**2)\n",
    "    count, bins_count = np.histogram(errors, bins=200)\n",
    "    pdf = count / sum(count)\n",
    "    cdf = np.cumsum(pdf)\n",
    "    \n",
    "    bins_count[0] = 0\n",
    "    cdf = np.append([0], cdf)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    if title is not None:\n",
    "        plt.title(title, fontsize=16)\n",
    "    plt.plot(bins_count, cdf)\n",
    "    plt.xlim((0, 2))\n",
    "    plt.xlabel(\"Absolute Localization Error [m]\")\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb89c9-eb5b-43c9-8712-d1740f0e384e",
   "metadata": {},
   "source": [
    "### Evaluate on training set itself (self-supervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dac63-64f6-43e8-a187-4f0e922f581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_chart_positions = cc_embmodel.predict(csi_time_domain_tensor)\n",
    "plot_colorized(channel_chart_positions, groundtruth_positions, title = \"Siamese-Based Channel Chart - Training Set\")\n",
    "plot_quiver(channel_chart_positions, groundtruth_positions, title = \"Training Set\")\n",
    "plot_cdf(channel_chart_positions, groundtruth_positions, title = \"Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca957cd-fd5e-4d38-8cda-57a4a22b426d",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d8e5c-274e-4bbb-8d7c-bcfce77cd90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_groundtruth_positions = []\n",
    "\n",
    "for csi, pos, time in test_set.prefetch(tf.data.AUTOTUNE).batch(1000):\n",
    "    test_set_groundtruth_positions.append(pos.numpy())\n",
    "\n",
    "test_set_groundtruth_positions = np.concatenate(test_set_groundtruth_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef547da8-5005-46a0-8331-6f10900a4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Channel Chart\n",
    "channel_chart_positions_testset = cc_embmodel.predict(test_set.prefetch(tf.data.AUTOTUNE).batch(1000))\n",
    "plot_colorized(channel_chart_positions_testset, test_set_groundtruth_positions, title = \"Siamese-Based Channel Chart - Test Set\")\n",
    "plot_quiver(channel_chart_positions_testset, test_set_groundtruth_positions, title = \"Training Set\")\n",
    "plot_cdf(channel_chart_positions_testset, test_set_groundtruth_positions, title = \"Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a8ff6-027c-48f1-892e-f65e9f778539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
